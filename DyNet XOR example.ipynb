{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The XOR problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, consider a model for solving the “xor” problem. The network has two inputs, which can be 0 or 1, and a single output which should be the xor of the two inputs. We will model this as a multi-layer perceptron with a single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynet as dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a parameter collection and add the parameters.\n",
    "Let x=x1,x2 be our input. We will have a hidden layer of 8 nodes, and an output layer of a single node. The activation on the hidden layer will be a tanh. Our network will then be:\n",
    "σ(V(tanh(Wx+b)))\n",
    "\n",
    "We want the output to be either 0 or 1, so we take the output layer to be the logistic-sigmoid function, σ(x), that takes values between −∞ and +∞ and returns numbers in [0,1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a parameter collection and populate it with parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.527769148349762,\n",
       " -0.5609636902809143,\n",
       " -0.5765582919120789,\n",
       " 0.08570045232772827,\n",
       " 0.5291308760643005,\n",
       " 0.08153563737869263,\n",
       " 0.46551305055618286,\n",
       " -0.03702342510223389]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = dy.ParameterCollection()\n",
    "W = m.add_parameters((8,2))  # 8x2 matrix\n",
    "V = m.add_parameters((1,8))  # 8x1 matrix\n",
    "b = m.add_parameters((8))  # 8-dim vector\n",
    "\n",
    "b.value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_dynet.ComputationGraph at 0x7f9d1ada4c60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy.renew_cg() # new computation graph. not strictly needed here, but good practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameters can be used as expressions in the computation graph. We now make use of V, W, and b in order to create the complete expression for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dy.vecInput(2) # an input vector of size 2. Also an expression.\n",
    "output = dy.logistic(V*(dy.tanh((W*x)+b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now query the (untrained) network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49302706122398376"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.set([0,0])\n",
    "output.value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to be able to define a loss, so we need an input expression to work against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6106782555580139\n",
      "0.7830334901809692\n"
     ]
    }
   ],
   "source": [
    "y = dy.scalarInput(0)  # this will hold the correct answer\n",
    "loss = dy.binary_log_loss(output, y)  # define the loss with respect to an output y\n",
    "\n",
    "x.set([1,0])\n",
    "y.set(0)\n",
    "print(loss.value())\n",
    "\n",
    "y.set(1)\n",
    "print(loss.value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to set the parameter weights such that the loss is minimized.\n",
    "\n",
    "For this, we will use a trainer object. A trainer is constructed with respect to the parameters of a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xor_instances(num_rounds=2000):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for round in range(num_rounds):\n",
    "        for x1 in 0,1:\n",
    "            for x2 in 0,1:\n",
    "                answer = 0 if x1==x2 else 1\n",
    "                questions.append((x1,x2))\n",
    "                answers.append(answer)\n",
    "    return questions, answers\n",
    "\n",
    "questions, answers = create_xor_instances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = dy.SimpleSGDTrainer(m)  # remember that m is the ParameterCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss is: 0.0018376447784248739\n",
      "average loss is: 0.00182388814719161\n",
      "average loss is: 0.0018103889521444217\n",
      "average loss is: 0.0017971337407652755\n",
      "average loss is: 0.001784119611256756\n",
      "average loss is: 0.0017713370975495006\n",
      "average loss is: 0.00175878117850516\n",
      "average loss is: 0.0017464446159283398\n",
      "average loss is: 0.0017343213751963857\n",
      "average loss is: 0.001722404639062006\n",
      "average loss is: 0.0017106894676213746\n",
      "average loss is: 0.001699170211601692\n",
      "average loss is: 0.001687841442001697\n",
      "average loss is: 0.001676698186908782\n",
      "average loss is: 0.0016657353423846265\n",
      "average loss is: 0.001654948228133435\n",
      "average loss is: 0.0016443321316996042\n",
      "average loss is: 0.001633883116786213\n",
      "average loss is: 0.0016235966343542954\n",
      "average loss is: 0.0016134688405436465\n",
      "average loss is: 0.0016034952484603438\n",
      "average loss is: 0.0015936724746345797\n",
      "average loss is: 0.001583997006249695\n",
      "average loss is: 0.0015744649877403086\n",
      "average loss is: 0.001565072685922496\n",
      "average loss is: 0.001555817284836219\n",
      "average loss is: 0.00154669539977072\n",
      "average loss is: 0.0015377037006380435\n",
      "average loss is: 0.0015288396597767633\n",
      "average loss is: 0.0015201000051844554\n",
      "average loss is: 0.001511482129534406\n",
      "average loss is: 0.0015029828871229256\n",
      "average loss is: 0.001494600174306525\n",
      "average loss is: 0.0014863308982218286\n",
      "average loss is: 0.001478172925650142\n",
      "average loss is: 0.0014701234573375485\n",
      "average loss is: 0.0014621807505987032\n",
      "average loss is: 0.001454342013304612\n",
      "average loss is: 0.0014466053039322083\n",
      "average loss is: 0.00143896832052269\n",
      "average loss is: 0.0014314290675371544\n",
      "average loss is: 0.0014239855873234948\n",
      "average loss is: 0.0014166357328703758\n",
      "average loss is: 0.0014093777816478077\n",
      "average loss is: 0.0014022097742741203\n",
      "average loss is: 0.0013951299674163633\n",
      "average loss is: 0.0013881366783530115\n",
      "average loss is: 0.001381228249186582\n",
      "average loss is: 0.0013744025018805049\n",
      "average loss is: 0.001367658641381422\n",
      "average loss is: 0.0013609945449427994\n",
      "average loss is: 0.0013544089729391718\n",
      "average loss is: 0.001347900026962205\n",
      "average loss is: 0.0013414667739103875\n",
      "average loss is: 0.0013351073547594504\n",
      "average loss is: 0.0013288206882602286\n",
      "average loss is: 0.0013226052255904824\n",
      "average loss is: 0.00131646000228196\n",
      "average loss is: 0.0013103834918297638\n",
      "average loss is: 0.0013043743268450877\n",
      "average loss is: 0.0012984316386797725\n",
      "average loss is: 0.001292554048850434\n",
      "average loss is: 0.0012867404348475653\n",
      "average loss is: 0.0012809895934015004\n",
      "average loss is: 0.0012753004095552919\n",
      "average loss is: 0.0012696721294813676\n",
      "average loss is: 0.0012641033597912555\n",
      "average loss is: 0.0012585932507713819\n",
      "average loss is: 0.0012531407837978686\n",
      "average loss is: 0.0012477449400799482\n",
      "average loss is: 0.0012424048712816496\n",
      "average loss is: 0.0012371196269547605\n",
      "average loss is: 0.0012318882855829106\n",
      "average loss is: 0.0012267098740621066\n",
      "average loss is: 0.0012215835266474945\n",
      "average loss is: 0.0012165086041472694\n",
      "average loss is: 0.001211484193132114\n",
      "average loss is: 0.0012065093769342639\n",
      "average loss is: 0.0012015834424454613\n",
      "average loss is: 0.0011967056295688963\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "seen_instances = 0\n",
    "\n",
    "for question, answer in zip(questions, answers):\n",
    "    x.set(question)\n",
    "    y.set(answer)\n",
    "    seen_instances += 1\n",
    "    total_loss += loss.value()\n",
    "    loss.backward()\n",
    "    trainer.update()\n",
    "\n",
    "    if (seen_instances > 1 and seen_instances % 100 == 0):\n",
    "        print(\"average loss is:\",total_loss / seen_instances)  # observe how in each iteration the loss gets smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The network is now trained. Let’s verify that it indeed learned the xor function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1 0.9992327690124512\n",
      "1,0 0.9989888072013855\n",
      "0,0 0.0003380033012945205\n",
      "1,1 0.0011132430518046021\n"
     ]
    }
   ],
   "source": [
    "x.set([0,1])\n",
    "print(\"0,1\", output.value())\n",
    "\n",
    "x.set([1,0])\n",
    "print(\"1,0\", output.value())\n",
    "\n",
    "x.set([0,0])\n",
    "print(\"0,0\", output.value())\n",
    "\n",
    "x.set([1,1])\n",
    "print(\"1,1\", output.value())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we are curious about the parameter values, we can query them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.35719442, -2.4858532 ],\n",
       "       [-0.31870559, -0.13677683],\n",
       "       [-2.44874334, -2.38197374],\n",
       "       [-1.51086211, -1.50610352],\n",
       "       [-1.20791674, -1.4196291 ],\n",
       "       [-2.87451792,  3.69964123],\n",
       "       [ 0.59257954,  0.49650541],\n",
       "       [-2.24656987,  1.25947213]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.58468819, -0.41393092, -3.70492268, -2.07314849,  2.64222288,\n",
       "        -5.93753099,  0.61036408,  2.10825324]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0862807035446167,\n",
       " -0.6335340142250061,\n",
       " 0.6043263077735901,\n",
       " -0.15373480319976807,\n",
       " 2.006375312805176,\n",
       " 1.2247034311294556,\n",
       " 0.8017867207527161,\n",
       " -0.428931325674057]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic networks are very similar to static ones, but instead of creating the network once and then calling “set” in each training example to change the inputs, we just create a new network for each training example.\n",
    "\n",
    "We present an example below. While the value of this may not be clear in the xor example, the dynamic approach is very convenient for networks for which the structure is not fixed, such as recurrent or recursive networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss is: 0.7246274840831757\n",
      "average loss is: 0.7049259021878242\n",
      "average loss is: 0.6761396740873654\n",
      "average loss is: 0.6295092997699976\n",
      "average loss is: 0.5688946435749531\n",
      "average loss is: 0.5064878949895502\n",
      "average loss is: 0.4511629040965012\n",
      "average loss is: 0.40475043436978014\n",
      "average loss is: 0.3662109077721834\n",
      "average loss is: 0.334039968220517\n",
      "average loss is: 0.30691831898452204\n",
      "average loss is: 0.28380593460518866\n",
      "average loss is: 0.26390510631725195\n",
      "average loss is: 0.24660537498870067\n",
      "average loss is: 0.2314363291611274\n",
      "average loss is: 0.21803172358020675\n",
      "average loss is: 0.2061031749472022\n",
      "average loss is: 0.19542103442932582\n",
      "average loss is: 0.1858004413378474\n",
      "average loss is: 0.1770910691670142\n",
      "average loss is: 0.1691695119431686\n",
      "average loss is: 0.1619335715574297\n",
      "average loss is: 0.15529792264427827\n",
      "average loss is: 0.14919079013906109\n",
      "average loss is: 0.14355137522779404\n",
      "average loss is: 0.1383278423553118\n",
      "average loss is: 0.13347573350062938\n",
      "average loss is: 0.12895670306120466\n",
      "average loss is: 0.12473750491819248\n",
      "average loss is: 0.1207891741503651\n",
      "average loss is: 0.1170863604272205\n",
      "average loss is: 0.11360678153840126\n",
      "average loss is: 0.11033077379279403\n",
      "average loss is: 0.1072409184081206\n",
      "average loss is: 0.10432173089456878\n",
      "average loss is: 0.10155939830900429\n",
      "average loss is: 0.09894156132453448\n",
      "average loss is: 0.09645712722950664\n",
      "average loss is: 0.09409611124533396\n",
      "average loss is: 0.09184950314369053\n",
      "average loss is: 0.08970914940423581\n",
      "average loss is: 0.0876676549761933\n",
      "average loss is: 0.08571829613693448\n",
      "average loss is: 0.08385494667899118\n",
      "average loss is: 0.08207201227938964\n",
      "average loss is: 0.08036437485392367\n",
      "average loss is: 0.07872734210429158\n",
      "average loss is: 0.0771566043830535\n",
      "average loss is: 0.07564819638674357\n",
      "average loss is: 0.07419846357405185\n",
      "average loss is: 0.07280403221258894\n",
      "average loss is: 0.07146178270488655\n",
      "average loss is: 0.07016882637484913\n",
      "average loss is: 0.06892248421618542\n",
      "average loss is: 0.06772026848005638\n",
      "average loss is: 0.06655986564184006\n",
      "average loss is: 0.06543912145341828\n",
      "average loss is: 0.06435602736286015\n",
      "average loss is: 0.06330870868965699\n",
      "average loss is: 0.062295413256195996\n",
      "average loss is: 0.06131450202174821\n",
      "average loss is: 0.06036443951641089\n",
      "average loss is: 0.05944378612714539\n",
      "average loss is: 0.05855119057767297\n",
      "average loss is: 0.057685383362838856\n",
      "average loss is: 0.05684517067144484\n",
      "average loss is: 0.056029428689698894\n",
      "average loss is: 0.055237098516937456\n",
      "average loss is: 0.05446718177022548\n",
      "average loss is: 0.053718736142701735\n",
      "average loss is: 0.052990871357705284\n",
      "average loss is: 0.052282745981145934\n",
      "average loss is: 0.0515935639515264\n",
      "average loss is: 0.0509225715112218\n",
      "average loss is: 0.0502690544786863\n",
      "average loss is: 0.04963233548769495\n",
      "average loss is: 0.04901177205444713\n",
      "average loss is: 0.04840675396922355\n",
      "average loss is: 0.047816701549697735\n",
      "average loss is: 0.04724106361591839\n"
     ]
    }
   ],
   "source": [
    "import dynet as dy\n",
    "\n",
    "def create_xor_instances(num_rounds=2000):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for round in range(num_rounds):\n",
    "        for x1 in 0,1:\n",
    "            for x2 in 0,1:\n",
    "                answer = 0 if x1==x2 else 1\n",
    "                questions.append((x1,x2))\n",
    "                answers.append(answer)\n",
    "    return questions, answers\n",
    "\n",
    "questions, answers = create_xor_instances()\n",
    "\n",
    "# create a network for the xor problem given input and output\n",
    "def create_xor_network(W, V, b, inputs, expected_answer):\n",
    "    dy.renew_cg() # new computation graph\n",
    "    x = dy.vecInput(len(inputs))\n",
    "    x.set(inputs)\n",
    "    y = dy.scalarInput(expected_answer)\n",
    "    output = dy.logistic(V*(dy.tanh((W*x)+b)))\n",
    "    loss =  dy.binary_log_loss(output, y)\n",
    "    return loss\n",
    "\n",
    "m2 = dy.ParameterCollection()\n",
    "W = m2.add_parameters((8,2))\n",
    "V = m2.add_parameters((1,8))\n",
    "b = m2.add_parameters((8))\n",
    "trainer = dy.SimpleSGDTrainer(m2)\n",
    "\n",
    "seen_instances = 0\n",
    "total_loss = 0\n",
    "for question, answer in zip(questions, answers):\n",
    "    loss = create_xor_network(W, V, b, question, answer)  # we create a new computation graph for each example \n",
    "    seen_instances += 1\n",
    "    total_loss += loss.value()\n",
    "    loss.backward()\n",
    "    trainer.update()\n",
    "    if (seen_instances > 1 and seen_instances % 100 == 0):\n",
    "        print(\"average loss is:\",total_loss / seen_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
